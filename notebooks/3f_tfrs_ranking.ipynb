{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde6b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from typing import Dict, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9974e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_DATA_PATH = './data/ratings.csv'\n",
    "USER_DATA_PATH = './data/users.csv'\n",
    "ITEM_DATA_PATH = './data/movies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d79497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ratings_df = pd.read_csv(RATINGS_DATA_PATH)\n",
    "ratings_df[\"user_id\"] = ratings_df[\"user_id\"].astype(str)  # for StringLookup\n",
    "ratings_df[\"title\"] = ratings_df[\"title\"].astype(str) # for StringLookup\n",
    "ratings_df[\"rating\"] = ratings_df[\"rating\"].astype(float)\n",
    "\n",
    "# convert to tf datasets\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings_df))\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"rating\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee5f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd52f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "movie_titles = ratings.batch(1_000_000).map(lambda x: x[\"movie_title\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370856a",
   "metadata": {},
   "source": [
    "### Implement Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0bda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking model\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 64\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_movie_titles, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.ratings = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "    \n",
    "  # def call(self, inputs):\n",
    "  #   user_id = inputs[\"user_id\"]\n",
    "  #   movie_title = inputs[\"movie_title\"]\n",
    "\n",
    "  #   user_embedding = self.user_embeddings(user_id)\n",
    "  #   movie_embedding = self.movie_embeddings(movie_title)\n",
    "\n",
    "  #   return self.ratings(tf.concat([user_embedding, movie_embedding], axis=1))\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    user_id, movie_title = inputs\n",
    "\n",
    "    user_embedding = self.user_embeddings(user_id)\n",
    "    movie_embedding = self.movie_embeddings(movie_title)\n",
    "\n",
    "    return self.ratings(tf.concat([user_embedding, movie_embedding], axis=1))\n",
    "  \n",
    "# full model\n",
    "class RecommendationModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ranking_model: tf.keras.Model = RankingModel()\n",
    "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "      loss = tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "    return self.ranking_model(\n",
    "        (features[\"user_id\"], features[\"movie_title\"]))\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    labels = features[\"user_rating\"]\n",
    "    inputs = {key: features[key] for key in features if key != \"user_rating\"}\n",
    "\n",
    "    rating_predictions = self(inputs)\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(labels=labels, predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a6703",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5f5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'recommendation_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 4.3333 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.6210 - total_loss: 4.3333\n",
      "Epoch 2/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2523 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1259 - total_loss: 1.2523\n",
      "Epoch 3/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.2184 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1093 - total_loss: 1.2184\n",
      "Epoch 4/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.1801 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0929 - total_loss: 1.1801\n",
      "Epoch 5/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1311 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0709 - total_loss: 1.1311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26066361550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = 0.1\n",
    "EPOCHS = 5\n",
    "\n",
    "model = RecommendationModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=LR))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31358b",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46eab04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1.1304 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0617 - total_loss: 1.1304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.1337839365005493>,\n",
       " 'root_mean_squared_error': <tf.Tensor: shape=(), dtype=float32, numpy=1.062831997871399>,\n",
       " 'regularization_loss': <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " 'total_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.1337839365005493>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018128e",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2409f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "M*A*S*H (1970): [[3.8083076]]\n",
      "Dances with Wolves (1990): [[3.7757146]]\n",
      "Speed (1994): [[3.7118106]]\n"
     ]
    }
   ],
   "source": [
    "test_ratings = {}\n",
    "test_movie_titles = [\"M*A*S*H (1970)\", \"Dances with Wolves (1990)\", \"Speed (1994)\"]\n",
    "for movie_title in test_movie_titles:\n",
    "  test_ratings[movie_title] = model({\n",
    "      \"user_id\": np.array([\"42\"]),\n",
    "      \"movie_title\": np.array([movie_title])\n",
    "  })\n",
    "\n",
    "print(\"Ratings:\")\n",
    "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
    "  print(f\"{title}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e72a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_movies_for_user(model, user_id: str, candidate_movies: list[str]) -> list[tuple[str, float]]:\n",
    "    # Repeat user_id for each movie\n",
    "    user_ids = tf.constant([user_id] * len(candidate_movies))\n",
    "    movie_titles = tf.constant(candidate_movies)\n",
    "\n",
    "    # Prepare input batch as dict\n",
    "    # inputs = {\n",
    "    #     \"user_id\": user_ids,\n",
    "    #     \"movie_title\": movie_titles\n",
    "    # }\n",
    "\n",
    "    # Run inference\n",
    "    # predictions = model.ranking_model(inputs)  # shape (N, 1)\n",
    "    predictions = model.ranking_model((user_ids, movie_titles))\n",
    "    predicted_ratings = tf.squeeze(predictions, axis=1).numpy()  # shape (N,)\n",
    "\n",
    "    # Zip movie titles with scores and sort\n",
    "    movie_scores = list(zip(candidate_movies, predicted_ratings))\n",
    "    ranked = sorted(movie_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de53c19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars (1977): 4.075\n",
      "Titanic (1997): 3.857\n",
      "Fargo (1996): 3.839\n",
      "Toy Story (1995): 3.745\n",
      "L.A. Confidential (1997): 3.723\n"
     ]
    }
   ],
   "source": [
    "candidate_titles = [\n",
    "    \"Star Wars (1977)\",\n",
    "    \"Toy Story (1995)\",\n",
    "    \"Fargo (1996)\",\n",
    "    \"L.A. Confidential (1997)\",\n",
    "    \"Titanic (1997)\"\n",
    "]\n",
    "\n",
    "user_id = \"82\"\n",
    "\n",
    "ranked_results = rank_movies_for_user(model, user_id, candidate_titles)\n",
    "\n",
    "for title, score in ranked_results:\n",
    "    print(f\"{title}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01567e70",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e4acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, \"export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2ec5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic (1997): 3.857\n",
      "Speed (1994): 3.594\n"
     ]
    }
   ],
   "source": [
    "# load the saved model and perform inference\n",
    "tfrs_model = tf.saved_model.load(\"export\")\n",
    "\n",
    "user_id = \"82\"\n",
    "candidate_movie_list = [\"Speed (1994)\", \"Titanic (1997)\"]\n",
    "\n",
    "ranked_recs = rank_movies_for_user(model, user_id, candidate_movie_list)\n",
    "\n",
    "for title, score in ranked_recs:\n",
    "    print(f\"{title}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f1b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf73f08",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84966f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_user_seen_dict(dataset):\n",
    "    user_seen = defaultdict(set)\n",
    "    for x in dataset:\n",
    "        user_seen[x[\"user_id\"].numpy().decode(\"utf-8\")].add(x[\"movie_title\"].numpy().decode(\"utf-8\"))\n",
    "    return user_seen\n",
    "\n",
    "train_user_seen = build_user_seen_dict(train)\n",
    "test_user_seen = build_user_seen_dict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c34696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking_model(model, test_user_seen, train_user_seen, all_movies, k=10):\n",
    "    hits, precision_sum, recall_sum, ndcg_sum = 0, 0.0, 0.0, 0.0\n",
    "    total_users = 0\n",
    "\n",
    "    for user_id, true_movies in test_user_seen.items():\n",
    "        # Remove movies already seen in training set\n",
    "        seen_train_movies = train_user_seen.get(user_id, set())\n",
    "        candidate_movies = [title for title in all_movies if title not in seen_train_movies]\n",
    "\n",
    "        # Get top-k predictions\n",
    "        ranked = rank_movies_for_user(model, user_id, candidate_movies)\n",
    "        top_k_preds = [title for title, score in ranked[:k]]\n",
    "\n",
    "        hit_set = true_movies & set(top_k_preds)\n",
    "        num_hits = len(hit_set)\n",
    "        hits += int(num_hits > 0)\n",
    "        precision_sum += num_hits / k\n",
    "        recall_sum += num_hits / len(true_movies)\n",
    "\n",
    "        # NDCG@k\n",
    "        dcg = 0.0\n",
    "        for i, movie in enumerate(top_k_preds):\n",
    "            if movie in true_movies:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(true_movies), k)))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        ndcg_sum += ndcg\n",
    "\n",
    "        total_users += 1\n",
    "\n",
    "    return {\n",
    "        'HitRate@k': hits / total_users,\n",
    "        'Precision@k': precision_sum / total_users,\n",
    "        'Recall@k': recall_sum / total_users,\n",
    "        'NDCG@k': ndcg_sum / total_users\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b988414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique movie titles from the dataset\n",
    "movies_df = pd.read_csv(ITEM_DATA_PATH)\n",
    "all_movie_titles = list(set(movies_df[\"title\"].tolist()))\n",
    "\n",
    "# Evaluate\n",
    "K = 10\n",
    "metrics = evaluate_ranking_model(\n",
    "    model=tfrs_model,\n",
    "    test_user_seen=test_user_seen,\n",
    "    train_user_seen=train_user_seen,\n",
    "    all_movies=all_movie_titles,\n",
    "    k=K\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Ranking Model Evaluation (k={K}):\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4586f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
