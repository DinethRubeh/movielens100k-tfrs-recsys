{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde6b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from typing import Dict, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9974e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DATA_PATH = './data/feature_store'\n",
    "\n",
    "USER_DATA_PATH = f'{PREPROCESSED_DATA_PATH}/users.csv'\n",
    "ITEM_DATA_PATH = f'{PREPROCESSED_DATA_PATH}/movies.csv'\n",
    "RATINGS_DATA_PATH = f'{PREPROCESSED_DATA_PATH}/ratings.csv'\n",
    "\n",
    "MODEL_PATH = './model/tfrs-ranking-with-sf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baeab60",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd83ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user data\n",
    "users_df = pd.read_csv(USER_DATA_PATH)\n",
    "\n",
    "# cast data type\n",
    "users_df[\"user_id\"] = users_df[\"user_id\"].astype(str)\n",
    "users_df[\"age\"] = users_df[\"age\"].astype(str)\n",
    "users_df[\"gender\"] = users_df[\"gender\"].astype(str)\n",
    "users_df[\"occupation\"] = users_df[\"occupation\"].astype(str)\n",
    "# select only the required columns\n",
    "users_df = users_df[['user_id', 'age', 'gender', 'occupation']]\n",
    "\n",
    "# movie data\n",
    "movies_df = pd.read_csv(ITEM_DATA_PATH)\n",
    "\n",
    "# cast data type\n",
    "movies_df[\"title\"] = movies_df[\"title\"].astype(str)\n",
    "movies_df[\"genres\"] = movies_df[\"title\"].astype(str)\n",
    "# select only the required columns\n",
    "movies_df = movies_df[['movie_id', 'title', 'genres', 'year']]\n",
    "\n",
    "# ratings data\n",
    "ratings_df = pd.read_csv(RATINGS_DATA_PATH)\n",
    "\n",
    "# cast data type\n",
    "ratings_df[\"user_id\"] = ratings_df[\"user_id\"].astype(str)  # for StringLookup\n",
    "ratings_df[\"title\"] = ratings_df[\"title\"].astype(str) # for StringLookup\n",
    "ratings_df[\"rating\"] = ratings_df[\"rating\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0e9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# users_df = pd.read_csv(USER_DATA_PATH)\n",
    "# movies_df = pd.read_csv(ITEM_DATA_PATH)\n",
    "# ratings_df = pd.read_csv(RATINGS_DATA_PATH)\n",
    "\n",
    "# # create user features\n",
    "# users_df[\"user_id\"] = users_df[\"user_id\"].astype(str)\n",
    "# users_df[\"age\"] = users_df[\"age\"].astype(str)  # treat as categorical\n",
    "# users_df[\"gender\"] = users_df[\"gender\"].astype(str)\n",
    "# users_df[\"occupation\"] = users_df[\"occupation\"].astype(str)\n",
    "\n",
    "# users_df = users_df[['user_id', 'age', 'gender', 'occupation']]\n",
    "\n",
    "# # create item features\n",
    "# movies_df[\"title\"] = movies_df[\"title\"].astype(str)\n",
    "\n",
    "# # combine genres into one string label per movie\n",
    "# genre_cols = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', \n",
    "#             'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', \n",
    "#             'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "# movies_df[\"genres\"] = movies_df[genre_cols].apply(\n",
    "#     lambda row: \"|\".join([genre for genre, val in row.items() if val == 1]), axis=1)\n",
    "\n",
    "# # extract release year\n",
    "# movies_df[\"year\"] = movies_df[\"release_date\"].str.extract(r\"(\\d{4})\").fillna(\"unknown\")\n",
    "\n",
    "# movies_df = movies_df[['movie_id', 'title', 'genres', 'year']]\n",
    "\n",
    "# # ratings: join user and movie side features to ratings_df\n",
    "# ratings_df[\"user_id\"] = ratings_df[\"user_id\"].astype(str)  # for StringLookup\n",
    "# ratings_df[\"title\"] = ratings_df[\"title\"].astype(str) # for StringLookup\n",
    "# ratings_df[\"rating\"] = ratings_df[\"rating\"].astype(float)\n",
    "# ratings_df = ratings_df.merge(users_df, on=\"user_id\", how=\"left\")\n",
    "# ratings_df = ratings_df.merge(movies_df[[\"title\", \"genres\", \"year\"]], on=\"title\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d79497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tf datasets\n",
    "ratings = tf.data.Dataset.from_tensor_slices(dict(ratings_df))\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"rating\"],\n",
    "    \"gender\": x[\"gender\"],\n",
    "    \"occupation\": x[\"occupation\"],\n",
    "    \"genres\": x[\"genres\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7829b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.map_op._MapDataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee5f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd52f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "movie_titles = ratings.batch(1_000_000).map(lambda x: x[\"movie_title\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "unique_genders = ratings_df[\"gender\"].dropna().unique().tolist()\n",
    "unique_occupations = ratings_df[\"occupation\"].dropna().unique().tolist()\n",
    "unique_ages = ratings_df[\"age\"].dropna().unique().tolist()\n",
    "unique_genres = ratings_df[\"genres\"].dropna().unique().tolist()\n",
    "unique_years = ratings_df[\"year\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370856a",
   "metadata": {},
   "source": [
    "### Implement Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0bda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie model\n",
    "class MovieModel(tf.keras.Model):\n",
    "    def __init__(self, unique_movie_titles, unique_genres):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        # Movie title embedding\n",
    "        self.title_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=unique_movie_titles, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        # Movie title text embedding\n",
    "        self.title_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "            self.title_vectorizer,\n",
    "            tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "        # Genre text vectorization and embedding\n",
    "        self.genre_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "        self.genre_embedding = tf.keras.Sequential([\n",
    "            self.genre_vectorizer,\n",
    "            tf.keras.layers.Embedding(max_tokens, 16, mask_zero=True),\n",
    "            tf.keras.layers.GlobalAveragePooling1D()\n",
    "        ])\n",
    "\n",
    "        # Adapt vectorizers\n",
    "        self.genre_vectorizer.adapt(unique_genres)\n",
    "        self.title_vectorizer.adapt(unique_movie_titles)\n",
    "\n",
    "    def call(self, inputs):  # inputs is a dict with 'movie_title' and 'genres'\n",
    "        return tf.concat([\n",
    "            self.title_embedding(inputs[\"movie_title\"]),\n",
    "            self.title_text_embedding(inputs[\"movie_title\"]),\n",
    "            self.genre_embedding(inputs[\"genres\"])\n",
    "        ], axis=1)\n",
    "\n",
    "# user model\n",
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, unique_user_ids, unique_genders, unique_occupations):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32)\n",
    "        ])\n",
    "\n",
    "        self.gender_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=unique_genders, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_genders) + 1, 8)\n",
    "        ])\n",
    "\n",
    "        self.occupation_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=unique_occupations, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_occupations) + 1, 8)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):  # inputs is a dict with 'user_id', 'gender', 'occupation'\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.gender_embedding(inputs[\"gender\"]),\n",
    "            self.occupation_embedding(inputs[\"occupation\"])\n",
    "        ], axis=1)\n",
    "\n",
    "\n",
    "# ranking model\n",
    "class RankingModel(tf.keras.Model):\n",
    "    def __init__(self, user_model, movie_model):\n",
    "        super().__init__()\n",
    "        self.user_model = user_model\n",
    "        self.movie_model = movie_model\n",
    "\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_repr = self.user_model({\n",
    "            \"user_id\": inputs[\"user_id\"],\n",
    "            \"gender\": inputs[\"gender\"],\n",
    "            \"occupation\": inputs[\"occupation\"]\n",
    "        })\n",
    "\n",
    "        movie_repr = self.movie_model({\n",
    "            \"movie_title\": inputs[\"movie_title\"],\n",
    "            \"genres\": inputs[\"genres\"]\n",
    "        })\n",
    "\n",
    "        return self.mlp(tf.concat([user_repr, movie_repr], axis=1))\n",
    "  \n",
    "# full model\n",
    "class RecommendationModel(tfrs.models.Model):\n",
    "    def __init__(self, ranking_model):\n",
    "        super().__init__()\n",
    "        self.ranking_model = ranking_model\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        return self.ranking_model(features)\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        labels = features[\"user_rating\"]\n",
    "        inputs = {key: features[key] for key in features if key != \"user_rating\"}\n",
    "\n",
    "        rating_predictions = self(inputs)\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(labels=labels, predictions=rating_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a6703",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b5f5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'recommendation_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - loss: 4.2733 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 2.6436 - total_loss: 4.2733\n",
      "Epoch 2/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1.2883 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1354 - total_loss: 1.2883\n",
      "Epoch 3/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1.2743 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1356 - total_loss: 1.2743\n",
      "Epoch 4/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 1.2388 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1187 - total_loss: 1.2388\n",
      "Epoch 5/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1.2152 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.1072 - total_loss: 1.2152\n",
      "Epoch 6/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1.1956 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0980 - total_loss: 1.1956\n",
      "Epoch 7/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1.1757 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0888 - total_loss: 1.1757\n",
      "Epoch 8/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 1.1548 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0790 - total_loss: 1.1548\n",
      "Epoch 9/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1.1331 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0687 - total_loss: 1.1331\n",
      "Epoch 10/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1.1105 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0579 - total_loss: 1.1105\n",
      "Epoch 11/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 1.0868 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0464 - total_loss: 1.0868\n",
      "Epoch 12/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1.0628 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0346 - total_loss: 1.0628\n",
      "Epoch 13/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1.0398 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0230 - total_loss: 1.0398\n",
      "Epoch 14/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 1.0188 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0122 - total_loss: 1.0188\n",
      "Epoch 15/15\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.9999 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0024 - total_loss: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x212c61e1690>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = 0.05\n",
    "EPOCHS = 15\n",
    "\n",
    "user_model = UserModel(unique_user_ids, unique_genders, unique_occupations)\n",
    "movie_model = MovieModel(unique_movie_titles, unique_genres)\n",
    "ranking_model = RankingModel(user_model, movie_model)\n",
    "model = RecommendationModel(ranking_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=LR))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31358b",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46eab04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 1.0204 - regularization_loss: 0.0000e+00 - root_mean_squared_error: 1.0099 - total_loss: 1.0204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0102993249893188>,\n",
       " 'root_mean_squared_error': <tf.Tensor: shape=(), dtype=float32, numpy=1.011264443397522>,\n",
       " 'regularization_loss': <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " 'total_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0102993249893188>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018128e",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e72a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_GENDER = \"M\",\n",
    "DEFAULT_OCCUPATION = \"other\",\n",
    "DEFAULT_GENRE = \"unknown\"\n",
    "\n",
    "def rank_movies_for_user(model, user_id:str, candidate_titles:list) -> list:\n",
    "    # if user_id not found\n",
    "    if user_id not in users_df[\"user_id\"].values:\n",
    "        raise ValueError(f\"user_id {user_id} not found\")\n",
    "\n",
    "    # Get user side features or fallback to default feature values\n",
    "    user_row = users_df[users_df[\"user_id\"] == user_id]\n",
    "    if not user_row.empty:\n",
    "        gender = user_row[\"gender\"].iloc[0] if pd.notna(user_row[\"gender\"].iloc[0]) else DEFAULT_GENDER\n",
    "        occupation = user_row[\"occupation\"].iloc[0] if pd.notna(user_row[\"occupation\"].iloc[0]) else DEFAULT_OCCUPATION\n",
    "    else:\n",
    "        gender = DEFAULT_GENDER\n",
    "        occupation = DEFAULT_OCCUPATION\n",
    "\n",
    "    movie_titles = []\n",
    "    movie_genres = []\n",
    "\n",
    "    for title in candidate_titles:\n",
    "        movie_row = movies_df[movies_df[\"title\"] == title]\n",
    "        genre = (\n",
    "            movie_row[\"genres\"].iloc[0] if not movie_row.empty and pd.notna(movie_row[\"genres\"].iloc[0])\n",
    "            else DEFAULT_GENRE\n",
    "        )\n",
    "        movie_titles.append(title)\n",
    "        movie_genres.append(genre)\n",
    "\n",
    "    n = len(candidate_titles)\n",
    "    model_inputs = {\n",
    "        \"user_id\": tf.constant([user_id] * n),\n",
    "        \"gender\": tf.constant([gender] * n),\n",
    "        \"occupation\": tf.constant([occupation] * n),\n",
    "        \"movie_title\": tf.constant(movie_titles),\n",
    "        \"genres\": tf.constant(movie_genres)\n",
    "    }\n",
    "\n",
    "    # Predict\n",
    "    predictions = model(model_inputs)\n",
    "    scores = tf.squeeze(predictions, axis=1).numpy()\n",
    "\n",
    "    # Rank\n",
    "    ranked = sorted(zip(candidate_titles, scores), key=lambda x: x[1], reverse=True)\n",
    "    ranked = [{\"movie_title\": title, \"score\": round(float(score), 4)} for title, score in ranked] # return a list of dicts\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de53c19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'movie_title': 'Star Wars (1977)', 'score': 4.102}, {'movie_title': 'Titanic (1997)', 'score': 3.9679}, {'movie_title': 'Fargo (1996)', 'score': 3.9383}, {'movie_title': 'L.A. Confidential (1997)', 'score': 3.8208}, {'movie_title': 'Toy Story (1995)', 'score': 3.7696}]\n"
     ]
    }
   ],
   "source": [
    "candidate_titles = [\n",
    "    \"Star Wars (1977)\",\n",
    "    \"Toy Story (1995)\",\n",
    "    \"Fargo (1996)\",\n",
    "    \"L.A. Confidential (1997)\",\n",
    "    \"Titanic (1997)\"\n",
    "]\n",
    "\n",
    "user_id = \"82\"\n",
    "\n",
    "ranked_results = rank_movies_for_user(model, user_id, candidate_titles)\n",
    "print(ranked_results)\n",
    "# for title, score in ranked_results:\n",
    "#     print(f\"{title}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01567e70",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03e4acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/tfrs-ranking-with-sf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/tfrs-ranking-with-sf\\assets\n"
     ]
    }
   ],
   "source": [
    "@tf.function(input_signature=[\n",
    "    tf.TensorSpec([None], tf.string, name=\"user_id\"),\n",
    "    tf.TensorSpec([None], tf.string, name=\"gender\"),\n",
    "    tf.TensorSpec([None], tf.string, name=\"occupation\"),\n",
    "    tf.TensorSpec([None], tf.string, name=\"movie_title\"),\n",
    "    tf.TensorSpec([None], tf.string, name=\"genres\"),\n",
    "])\n",
    "def serve_fn(user_id, gender, occupation, movie_title, genres):\n",
    "    features = {\n",
    "        \"user_id\": user_id,\n",
    "        \"gender\": gender,\n",
    "        \"occupation\": occupation,\n",
    "        \"movie_title\": movie_title,\n",
    "        \"genres\": genres,\n",
    "    }\n",
    "    return model(features)\n",
    "\n",
    "tf.saved_model.save(model, export_dir=MODEL_PATH, signatures={\"serving_default\": serve_fn})\n",
    "\n",
    "# tf.saved_model.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d690b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_GENDER = \"M\"\n",
    "DEFAULT_OCCUPATION = \"other\"\n",
    "DEFAULT_GENRE = \"unknown\"\n",
    "\n",
    "def rank_movies_for_user_with_loaded_model(model, user_id: str, candidate_titles: list) -> list:\n",
    "    # Check if user_id exists\n",
    "    if user_id not in users_df[\"user_id\"].values:\n",
    "        raise ValueError(f\"user_id {user_id} not found\")\n",
    "\n",
    "    # Get user side features or use defaults\n",
    "    user_row = users_df[users_df[\"user_id\"] == user_id]\n",
    "    gender = user_row[\"gender\"].iloc[0] if not user_row.empty and pd.notna(user_row[\"gender\"].iloc[0]) else DEFAULT_GENDER\n",
    "    occupation = user_row[\"occupation\"].iloc[0] if not user_row.empty and pd.notna(user_row[\"occupation\"].iloc[0]) else DEFAULT_OCCUPATION\n",
    "\n",
    "    # Prepare movie inputs\n",
    "    movie_titles = []\n",
    "    movie_genres = []\n",
    "    for title in candidate_titles:\n",
    "        movie_row = movies_df[movies_df[\"title\"] == title]\n",
    "        genre = (\n",
    "            movie_row[\"genres\"].iloc[0]\n",
    "            if not movie_row.empty and pd.notna(movie_row[\"genres\"].iloc[0])\n",
    "            else DEFAULT_GENRE\n",
    "        )\n",
    "        movie_titles.append(title)\n",
    "        movie_genres.append(genre)\n",
    "\n",
    "    n = len(candidate_titles)\n",
    "\n",
    "    # Run inference using individual keyword arguments\n",
    "    predictions = model(\n",
    "        user_id=tf.constant([user_id] * n),\n",
    "        gender=tf.constant([gender] * n),\n",
    "        occupation=tf.constant([occupation] * n),\n",
    "        movie_title=tf.constant(movie_titles),\n",
    "        genres=tf.constant(movie_genres)\n",
    "    )\n",
    "\n",
    "    scores = tf.squeeze(predictions[\"output_0\"], axis=1).numpy()\n",
    "\n",
    "    # Rank and return\n",
    "    ranked = sorted(zip(candidate_titles, scores), key=lambda x: x[1], reverse=True)\n",
    "    ranked = [{\"movie_title\": title, \"score\": round(float(score), 4)} for title, score in ranked]\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a1e051b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction (*, gender: TensorSpec(shape=(None,), dtype=tf.string, name='gender'), genres: TensorSpec(shape=(None,), dtype=tf.string, name='genres'), movie_title: TensorSpec(shape=(None,), dtype=tf.string, name='movie_title'), occupation: TensorSpec(shape=(None,), dtype=tf.string, name='occupation'), user_id: TensorSpec(shape=(None,), dtype=tf.string, name='user_id')) -> Dict[['output_0', TensorSpec(shape=(None, 1), dtype=tf.float32, name='output_0')]] at 0x212E682FB50>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the saved model and perform inference\n",
    "tfrs_model = tf.saved_model.load(MODEL_PATH).signatures[\"serving_default\"]\n",
    "tfrs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "291d1c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_0': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[3.6877604]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# Combine all features\n",
    "result = tfrs_model(\n",
    "    user_id=tf.constant([\"42\"]),\n",
    "    gender=tf.constant([\"F\"]),\n",
    "    occupation=tf.constant([\"student\"]),\n",
    "    movie_title=tf.constant([\"Titanic (1997)\"]),\n",
    "    genres=tf.constant([\"Action\"])\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a2ec5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'movie_title': 'Titanic (1997)', 'score': 4.3067}, {'movie_title': 'Speed (1994)', 'score': 3.8966}]\n"
     ]
    }
   ],
   "source": [
    "user_id = \"72\"\n",
    "candidate_movie_list = [\"Speed (1994)\", \"Titanic (1997)\"]\n",
    "\n",
    "ranked_recs = rank_movies_for_user_with_loaded_model(tfrs_model, user_id, candidate_movie_list)\n",
    "print(ranked_recs)\n",
    "# for title, score in ranked_recs:\n",
    "#     print(f\"{title}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7125a",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f04f1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_user_seen_dict(dataset):\n",
    "    user_seen = defaultdict(set)\n",
    "    for x in dataset:\n",
    "        user_seen[x[\"user_id\"].numpy().decode(\"utf-8\")].add(x[\"movie_title\"].numpy().decode(\"utf-8\"))\n",
    "    return user_seen\n",
    "\n",
    "train_user_seen = build_user_seen_dict(train)\n",
    "test_user_seen = build_user_seen_dict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c88f0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking_model(model, test_user_seen, train_user_seen, all_movies, k=10):\n",
    "    hits, precision_sum, recall_sum, ndcg_sum = 0, 0.0, 0.0, 0.0\n",
    "    total_users = 0\n",
    "\n",
    "    for user_id, true_movies in test_user_seen.items():\n",
    "        # Remove movies already seen in training set\n",
    "        seen_train_movies = train_user_seen.get(user_id, set())\n",
    "        candidate_movies = [title for title in all_movies if title not in seen_train_movies]\n",
    "\n",
    "        # Get top-k predictions\n",
    "        ranked = rank_movies_for_user(model, user_id, candidate_movies)\n",
    "        top_k_preds = [title for title, score in ranked[:k]]\n",
    "\n",
    "        hit_set = true_movies & set(top_k_preds)\n",
    "        num_hits = len(hit_set)\n",
    "        hits += int(num_hits > 0)\n",
    "        precision_sum += num_hits / k\n",
    "        recall_sum += num_hits / len(true_movies)\n",
    "\n",
    "        # NDCG@k\n",
    "        dcg = 0.0\n",
    "        for i, movie in enumerate(top_k_preds):\n",
    "            if movie in true_movies:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(true_movies), k)))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        ndcg_sum += ndcg\n",
    "\n",
    "        total_users += 1\n",
    "\n",
    "    return {\n",
    "        'HitRate@k': hits / total_users,\n",
    "        'Precision@k': precision_sum / total_users,\n",
    "        'Recall@k': recall_sum / total_users,\n",
    "        'NDCG@k': ndcg_sum / total_users\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4b952b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Model Evaluation (k=10):\n",
      "HitRate@k: 0.4846\n",
      "Precision@k: 0.1001\n",
      "Recall@k: 0.0445\n",
      "NDCG@k: 0.1236\n"
     ]
    }
   ],
   "source": [
    "# Get all unique movie titles from the dataset\n",
    "all_movie_titles = list(set(movies_df[\"title\"].tolist()))\n",
    "\n",
    "# Evaluate\n",
    "K = 10\n",
    "metrics = evaluate_ranking_model(\n",
    "    model=model,\n",
    "    test_user_seen=test_user_seen,\n",
    "    train_user_seen=train_user_seen,\n",
    "    all_movies=all_movie_titles,\n",
    "    k=K\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Ranking Model Evaluation (k={K}):\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21bcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
